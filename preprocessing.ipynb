{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing eye-tracking data\n",
    "\n",
    "#### ⚠️ Before you start, download the two dataset definition files `gctg.yaml` and `gctg-clean.yaml` from Moodle and put them in the root directory of this repository. ⚠️\n",
    "\n",
    "> Please don't upload/push/publish these files elsewhere, as the dataset is confidential.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook will cover:\n",
    "\n",
    "1. [Levels of preprocessing](#1-levels-of-preprocessing)\n",
    "2. [Inspecting and visualizing data](#2-inspecting-and-visualizing-data)\n",
    "3. [Cleaning up data](#3-cleaning-up-data)\n",
    "4. [Detecting events](#4-detecting-events)\n",
    "5. [Mapping fixations to AOIs](#5-mapping-fixations-to-aois)\n",
    "6. [Calculating reading measures](#6-calculating-reading-measures)\n",
    "\n",
    "We will make use of the Python library [pymovements](https://pymovements.readthedocs.io/).\n",
    "\n",
    "> **NOTE:** pymovements is still relatively young and under active development. Some features may be missing or buggy. You can get support during the seminar sessions and on the Moodle forum. If you notice any bugs or think that something could be improved in the library, don't hesitate to open an issue [on GitHub](https://github.com/aeye-lab/pymovements/issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymovements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Levels of preprocessing\n",
    "\n",
    "Generally, there are three levels of preprocessing eye-tracking-while-reading data:\n",
    "\n",
    "- **Raw data:** Eye movement data as it is recorded by the eye tracker -- usually between 500 and 2000 samples per second. Each samples include X/Y gaze coordinates and possibly other measurements like pupil size.\n",
    "- **Event data:** Automatically detected events like fixations and saccades. Each event has an onset and offset time; in combination with the raw data, properties like fixation duration, saccade amplitude, or maximum velocity can be calculated.\n",
    "- **Reading measures:** Scalar measures like first fixation duration, first-pass gaze duration, and skip rate, calculated for each area of interest (e.g., for each word).\n",
    "\n",
    "![Preprocessing pipeline](pipeline.drawio.png)\n",
    "\n",
    "Unfortunately, many [publicly available datasets](https://pymovements.readthedocs.io/en/stable/datasets/index.html#public-datasets) do not include the raw data. If you want to use a dataset for an experiment where you need to apply your own preprocessing pipeline, make sure to check the available data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Inspecting and visualizing data\n",
    "\n",
    "First, let's download the dataset and load it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymovements as pm\n",
    "\n",
    "dataset = pm.Dataset(\"gctg.yaml\", \"data\")\n",
    "dataset.download()\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains one `GazeDataFrame` per subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.gaze[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's split those tables so that there is one `GazeDataFrame` for each screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split_gaze_data(\"stimulus\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the gaze data from one subject on one of the screens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_gaze = dataset.gaze[0]\n",
    "pm.plotting.traceplot(stimulus_gaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very helpful, because we can't see the text that the subject was looking at.\n",
    "\n",
    "Let's extract the name of the stimulus and find the corresponding stimulus image in the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "stimulus_name = stimulus_gaze.frame[\"stimulus\"].unique().item()\n",
    "stimulus_path = Path(\"data\", \"raw\", \"stimuli\", f\"{stimulus_name}.png\")\n",
    "\n",
    "stimulus_name, stimulus_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the stimulus image to the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plotting.traceplot(stimulus_gaze, add_stimulus=True, path_to_image_stimulus=stimulus_path)\n",
    "stimulus_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the calibration towards the bottom of the screen appears to be a bit off (we will fix this in step 3 below).\n",
    "\n",
    "Data in pymovements is stored as [polars](https://pola.rs/) data frames. Polars is a library similar to pandas, but is generally faster and provides a more functional-programming-like interface.\n",
    "\n",
    "You can access the raw data using the `frame` attribute and use it, for example, to create your own plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pupil_size = stimulus_gaze.frame[\"pupil\"]\n",
    "pixel_x = stimulus_gaze.frame.select(pl.col(\"pixel\").list.first())\n",
    "pixel_y = stimulus_gaze.frame.select(pl.col(\"pixel\").list.last())\n",
    "\n",
    "fig, (ax_x, ax_y, ax_pupil) = plt.subplots(3, 1, figsize=(10, 6), sharex=True)\n",
    "ax_x.plot(pixel_x)\n",
    "ax_y.plot(pixel_y)\n",
    "ax_pupil.plot(pupil_size)\n",
    "ax_x.set_ylabel(\"X location (pixel)\")\n",
    "ax_y.set_ylabel(\"Y location (pixel)\")\n",
    "ax_pupil.set_ylabel(\"Pupil size\")\n",
    "ax_pupil.set_xlabel(\"Time (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many samples we have per screen and by how many subjects each screen has been seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pl.concat([gaze.frame for gaze in dataset.gaze])\n",
    "all_samples.group_by(\"stimulus\").agg(\n",
    "    [\n",
    "        pl.col(\"time\").count().alias(\"num_samples\"),\n",
    "        pl.col(\"subject_id\").unique().count().alias(\"num_subjects\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one screen which was accidentally skipped during the experiment. Can you find it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the stimulus with fewer than 4 subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning up data\n",
    "\n",
    "Which data cleaning steps are necessary highly depends on the use case. For reading experiments, this commonly includes:\n",
    "\n",
    "- Correcting sample or fixation locations in case of bad calibration\n",
    "- Removing the first and last fixations on a page or line (because there is often a bit of \"random\" movement)\n",
    "- Removing blinks and other noise\n",
    "\n",
    "Here, we will only look at manual correction of sample locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `correct.py` module implements a simple graphical interface for moving and warping raw gaze data. You can open a CSV file and edit one screen at a time.\n",
    "\n",
    "- **Left mouse button:** Add and move anchor points\n",
    "- **Right mouse button:** Remove anchor points\n",
    "- `→`: Next screen\n",
    "- `←`: Previous screen\n",
    "- `CTRL+Z`: Undo\n",
    "- `ESC`: Save and exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import correct\n",
    "\n",
    "correct.main(Path(\"data\", \"raw\", \"gaze\", \"P01.csv\"), vertical=True)\n",
    "# vertical=True means that you will only correct the vertical location (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the CLI from a terminal:\n",
    "\n",
    "```bash\n",
    "python correct.py data/raw/gaze/P01.csv --vertical\n",
    "```\n",
    "\n",
    "When you are done correcting, you can apply the transformations, which will create a new CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apply_transforms\n",
    "\n",
    "apply_transforms.main(Path(\"data\", \"raw\", \"gaze\", \"P01.csv\"))\n",
    "# This will create P01.corrected.csv in the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a version of the dataset with manually corrected raw data, which you can use for your experiments. This version only includes the **text screens for the experimental trials**. If you want to use gaze data on the question screens or the practice trial, you need to get it from the uncleaned dataset, and possibly manually correct them first.\n",
    "\n",
    "You can fetch and load the cleaned dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pm.Dataset(\"gctg-clean.yaml\", \"data-clean\").download().load()\n",
    "dataset.split_gaze_data(\"stimulus\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some trials, calibration quality was too low to be adequately corrected, so we excluded them. (But note that we only removed the gaze data, not the response data.)\n",
    "\n",
    "You can find the missing trials like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pl.concat([gaze.frame for gaze in dataset.gaze])\n",
    "all_samples.group_by(\"stimulus\").agg(\n",
    "    [\n",
    "        pl.col(\"time\").count().alias(\"num_samples\"),\n",
    "        pl.col(\"subject_id\").unique().count().alias(\"num_subjects\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detecting events\n",
    "\n",
    "Depending on the research question, different events may be of interest:\n",
    "\n",
    "- Fixations\n",
    "- Saccades\n",
    "- Microsaccades\n",
    "- Smooth pursuit\n",
    "- Blinks\n",
    "- ...\n",
    "\n",
    "For calculating reading measures, fixations are the most relevant. Two algorithms are commonly used for fixation detection:\n",
    "\n",
    "- **IVT:** identification by velocity threshold; detects a fixation as long as the velocity is below a predefined threshold\n",
    "- **IDT:** identification by dispersion threshold; detects a fixation as long as the gaze location does not move further than a predefined distance\n",
    "\n",
    "Both IVT and IDT use the angular gaze position, so we first need to convert the pixel coordinates to degrees of visual angle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.pix2deg()\n",
    "\n",
    "stimulus_gaze = dataset.gaze[0]\n",
    "pixel_x = stimulus_gaze.frame.select(pl.col(\"pixel\").list.first())[:5000]\n",
    "pixel_y = stimulus_gaze.frame.select(pl.col(\"pixel\").list.last())[:5000]\n",
    "position_x = stimulus_gaze.frame.select(pl.col(\"position\").list.first())[:5000]\n",
    "position_y = stimulus_gaze.frame.select(pl.col(\"position\").list.last())[:5000]\n",
    "\n",
    "fig, (ax_x, ax_y, ax_px, ax_py) = plt.subplots(4, 1, figsize=(10, 6), sharex=True)\n",
    "ax_x.plot(pixel_x)\n",
    "ax_y.plot(pixel_y)\n",
    "ax_px.plot(position_x)\n",
    "ax_py.plot(position_y)\n",
    "ax_x.set_ylabel(\"X location [pix]\")\n",
    "ax_y.set_ylabel(\"Y location [pix]\")\n",
    "ax_px.set_ylabel(\"X position [°]\")\n",
    "ax_py.set_ylabel(\"Y position [°]\")\n",
    "ax_py.set_xlabel(\"Time [ms]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IVT, we also need to calculate the velocity (in degrees per second). Due to random noise, the signal constantly jumps around a bit, even if the eye is completely still. Therefore, smoothing is usually applied when calculating the velocity. You can find the different methods implemented in pymovements [here](https://pymovements.readthedocs.io/en/stable/reference/pymovements.gaze.transforms.pos2vel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.pos2vel(method=\"fivepoint\")\n",
    "\n",
    "stimulus_gaze = dataset.gaze[0]\n",
    "position_x = stimulus_gaze.frame.select(pl.col(\"position\").list.first())[:5000]\n",
    "position_y = stimulus_gaze.frame.select(pl.col(\"position\").list.last())[:5000]\n",
    "velocity_x = stimulus_gaze.frame.select(pl.col(\"velocity\").list.first())[:5000]\n",
    "velocity_y = stimulus_gaze.frame.select(pl.col(\"velocity\").list.last())[:5000]\n",
    "\n",
    "fig, (ax_x, ax_y, ax_vx, ax_vy) = plt.subplots(4, 1, figsize=(10, 6), sharex=True)\n",
    "ax_x.plot(position_x)\n",
    "ax_y.plot(position_y)\n",
    "ax_vx.plot(velocity_x)\n",
    "ax_vy.plot(velocity_y)\n",
    "ax_x.set_ylabel(\"X position [°]\")\n",
    "ax_y.set_ylabel(\"Y position [°]\")\n",
    "ax_vx.set_ylabel(\"X velocity [°/s]\")\n",
    "ax_vy.set_ylabel(\"Y velocity [°/s]\")\n",
    "ax_vy.set_xlabel(\"Time [ms]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use IDT or IVT to detect fixations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.detect(\"idt\", clear=True)\n",
    "dataset.events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duration of each event is automatically calculated, but the fixation locations are missing. Let's add them by calculating the average pixel coordinates for each fixation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_properties((\"location\", {\"position_column\": \"pixel\"}))\n",
    "dataset.gaze[0].events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the detected events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_gaze = dataset.gaze[0]\n",
    "stimulus_name = stimulus_gaze.frame[\"stimulus\"].unique().item()\n",
    "\n",
    "stimulus_path = Path(\"data\", \"raw\", \"stimuli\", f\"{stimulus_name}.png\")\n",
    "pm.plotting.scanpathplot(stimulus_gaze.events, stimulus_gaze, add_traceplot=True, add_stimulus=True, path_to_image_stimulus=stimulus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data for a couple of screens and subjects. Are fixations detected reliably? If there are missing fixations, try tweaking the thresholds or using a different algorithm. You can find more information about [IDT](https://pymovements.readthedocs.io/en/stable/reference/pymovements.events.idt.html#pymovements.events.idt) and [IVT](https://pymovements.readthedocs.io/en/stable/reference/pymovements.events.ivt.html#pymovements.events.ivt) and its parameters in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mapping fixations to AOIs\n",
    "\n",
    "We would like to assign each fixation to a word-level area of interest. The AOI rectangles have been predefined in CSV files. We can load them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_names = all_samples[\"stimulus\"].unique()\n",
    "stimuli = {}\n",
    "for stimulus_name in stimulus_names:\n",
    "    aois_path = Path(\"data\", \"raw\", \"stimuli\", f\"{stimulus_name}.word.csv\")\n",
    "    aois = pl.read_csv(aois_path)\n",
    "    stimulus = pm.stimulus.TextStimulus(\n",
    "        aois,\n",
    "        aoi_column=\"content\",\n",
    "        start_x_column=\"left\",\n",
    "        start_y_column=\"top\",\n",
    "        end_x_column=\"right\",\n",
    "        end_y_column=\"bottom\",\n",
    "    )\n",
    "    stimuli[stimulus_name] = stimulus\n",
    "stimuli[\"goldfish-zero.text.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's map fixations to AOIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for events in dataset.events:\n",
    "    stimulus_name = events.frame[\"stimulus\"].unique().item()\n",
    "    events.map_to_aois(stimuli[stimulus_name])\n",
    "\n",
    "dataset.events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculating reading measures\n",
    "\n",
    "Unfortunately, this final part of the pipeline is still underdeveloped in pymovements, so you will have to implement the reading measures you need yourself.\n",
    "\n",
    "Some things to watch out for:\n",
    "- If you want to calculate reading measures for every word, make sure you include words that are never fixated by anyone (i.e., set the reading measures to 0).\n",
    "- Make sure to exclude missing trials/screens (i.e., *don't* set the reading measures to 0 in that case).\n",
    "\n",
    "Here is a simple example for total reading time (i.e., the sum of durations of all fixations on an AOI), applied to one screen and subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = dataset.events[0].frame\n",
    "\n",
    "reading_measures = (\n",
    "    events\n",
    "    # Remove fixations that are not located in any AOI\n",
    "    .filter(pl.col(\"index\").is_not_null())\n",
    "    # Group by AOI index and aggregate\n",
    "    .group_by(\"index\")\n",
    "    .agg(\n",
    "        pl.col(\"subject_id\").first(),\n",
    "        pl.col(\"stimulus\").first(),\n",
    "        pl.col(\"content\").first(),\n",
    "        pl.col(\"duration\").sum().alias(\"total_reading_time\"),\n",
    "    )\n",
    "    # Sort by AOI index\n",
    "    .sort(\"index\")\n",
    ")\n",
    "reading_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, `reading_measures` only includes AOIs which have been fixated at some point. However, we want to calculate total reading time for *all* AOIs, even those that were never fixated (like the AOI with index 61, which should get a TRT of 0). We can achieve this by doing a *right join* between the reading measures and the AOIs:\n",
    "\n",
    "![Diagram of a right join](https://www.w3schools.com/sql/img_right_join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = events[\"subject_id\"].unique().item()\n",
    "stimulus_name = events[\"stimulus\"].unique().item()\n",
    "stimulus_aois = stimuli[stimulus_name].aois\n",
    "\n",
    "reading_measures = reading_measures.join(\n",
    "    stimulus_aois,\n",
    "    on=[\"index\", \"content\"],\n",
    "    how=\"right\",\n",
    ").select(pl.col(\"subject_id\"), pl.col(\"stimulus\"), pl.col(\"index\"), pl.col(\"content\"), pl.col(\"total_reading_time\"))\n",
    "\n",
    "# Fill missing values that were not present in the AOI table\n",
    "reading_measures = reading_measures.with_columns(\n",
    "    pl.col(\"subject_id\").fill_null(subject_id),\n",
    "    pl.col(\"stimulus\").fill_null(stimulus_name),\n",
    "    pl.col(\"total_reading_time\").fill_null(0),\n",
    ")\n",
    "reading_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calculating some more complex reading measures, like first-pass gaze duration or regression rate (0 if there is no regression from this AOI, 1 if there is).\n",
    "\n",
    "If you're not comfortable with polars, you can also convert the data to a pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate more reading measures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
