{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfa679e",
   "metadata": {},
   "source": [
    "# Extracting \"cognitive\" signals from language models\n",
    "\n",
    "This notebook will cover:\n",
    "\n",
    "1. [Extracting surprisal](#1-extracting-surprisal)\n",
    "2. [Extracting attention scores](#2-extracting-attention-scores)\n",
    "3. [Extracting word embeddings](#3-extracting-word-embeddings)\n",
    "\n",
    "We will use Hugging Face's [transformers](https://huggingface.co/docs/transformers/en/index) library.\n",
    "\n",
    "The techniques shown in this notebook only do a single forward pass through the model, and the models are quite small, so you should be able to run everything on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b48826",
   "metadata": {},
   "source": [
    "## 1. Extracting surprisal\n",
    "\n",
    "Surprisal is the negative log-probability of a word given its left-hand context:\n",
    "\n",
    "$surprisal(w_i) = -\\log(P(w_i | w_1, w_2 \\dots, w_{i-1}))$\n",
    "\n",
    "In GPT-style transformer language models, the attention on the right-hand context is masked for every token, therefore the output probabilities for each token are only based on the left-hand context. This means that we can do a single forward pass with the entire text and get the surprisal values for each token at once, instead of having to do a forward pass for every single token.\n",
    "\n",
    "Let's test this with GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"Words are flowing out like endless rain into a paper cup\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output logits and convert to log-probabilities\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits[0]\n",
    "logprobs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "# Get the log-probabilities of the true next tokens and convert to surprisals\n",
    "next_token_input_ids = inputs.input_ids[0, 1:]\n",
    "next_token_logprobs = logprobs.gather(1, next_token_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "surprisal = -next_token_logprobs\n",
    "\n",
    "# Print the surprisal values\n",
    "for token_id, surprisal_value in zip(next_token_input_ids, surprisal):\n",
    "    token = tokenizer.decode(token_id)\n",
    "    print(f\"{token!r}\\t{surprisal_value.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bccf0",
   "metadata": {},
   "source": [
    "Note that we don't get a surprisal value for the first word. This is because GPT-2 was trained without a BOS token. The first word in the sequence is *Words*, therefore the first predicted next token is the second word in the sequence (*are*).\n",
    "\n",
    "Can you explain why certain words have high/low surprisal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055390d",
   "metadata": {},
   "source": [
    "## 2. Extracting attention scores\n",
    "\n",
    "Transformer language models usually have multiple layers and multiple attention heads within each layer. So for each token in the input sequence, we get a whole bunch of attention scores, making it more difficult to interpret them.\n",
    "\n",
    "GPT-2 has 12 layers and 12 attention heads per layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b721ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "for layer, attentions in enumerate(outputs.attentions, 1):\n",
    "    print(f\"Layer {layer}: {attentions.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411ab89",
   "metadata": {},
   "source": [
    "The first dimension in the tensors is the batch dimension. Can you explain what the other dimensions mean?\n",
    "\n",
    "To simplify things, let's average the attention scores across all layers and heads and visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a74950",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attentions = torch.concatenate(outputs.attentions, dim=0).mean(dim=(0, 1))\n",
    "mean_attentions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(mean_attentions.numpy(), cmap='gray')\n",
    "plt.colorbar()\n",
    "tokens = [tokenizer.decode(token_id) for token_id in inputs.input_ids[0]]\n",
    "plt.xticks(ticks=range(len(tokens)), labels=tokens, rotation=90)\n",
    "plt.yticks(ticks=range(len(tokens)), labels=tokens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b523c",
   "metadata": {},
   "source": [
    "Try to answer the following questions:\n",
    "\n",
    "- Which token(s) receive(s) the most attention?\n",
    "- Why is the upper right half completely black?\n",
    "- Do the attention patterns look similar in all layers and attention heads? Can you find heads or layers with more interesting patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94912fb0",
   "metadata": {},
   "source": [
    "## 3. Extracting word embeddings\n",
    "\n",
    "Like most modern NLP models that process text input, the very first layer in a transformer model is a regular embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bdf860",
   "metadata": {},
   "source": [
    "Can you figure out what this model's vocabulary size is? And what is its embedding size?\n",
    "\n",
    "While we *could* use the embeddings produced by the first layer as word representations, this is not be a very good representation of that word's meaning (can you explain why?). What people usually do is use hidden states from intermediate layers. Here's how you can access these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5051881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "for layer, hidden_states in enumerate(outputs.hidden_states, 1):\n",
    "    print(f\"Layer {layer}: {hidden_states.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1f998",
   "metadata": {},
   "source": [
    "Again, the first dimension is the batch dimension.\n",
    "\n",
    "Now, which layer(s) you use for your representation depends on your use case. The early layers tend to contain less information about context, while the very last layer mainly contains information about the next word to be predicted. A common approach is to average the last couple of layers (see, for example, Table 7 in the original BERT paper: [Devlin et al., 2019](https://doi.org/10.18653/v1/N19-1423)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17688cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outputs.hidden_states[-4:]  # Use the last 4 layers\n",
    "mean_embedding = torch.mean(torch.concatenate(embeddings), dim=0)\n",
    "mean_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98cbcd",
   "metadata": {},
   "source": [
    "We have now extracted word embeddings from GPT-2. Since GPT-2 is a unidirectional language model, each embedding will only contain information about the word's left-hand context. To get bidirectionally contextualized embeddings, we would have to use a bidirectional model like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a750943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a BERT model to get bidirectionally contextualized embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
